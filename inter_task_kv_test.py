#!/usr/bin/env python3
"""
Inter-Task KV Reuse Test Script
Based on llama_lsh_args.py pattern
Tests the KV cache reuse functionality with detailed logging
"""

import argparse
import gc
import torch
import numpy as np
import time
import sys
import os
from transformers import AutoTokenizer
from pathlib import Path

import shutil

# åœ¨mainå‡½æ•°å‰å®šä¹‰è§£æå‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def parse_args():
    parser = argparse.ArgumentParser(description="Inter-Task KV Reuse Test Script")
    parser.add_argument("--ttft_test", type=bool, default=False, help="Enable TTFT latency test")
    parser.add_argument("--f1_test", type=bool, default=False, help="Enable F1 score test")
    parser.add_argument("--cache_test", type=bool, default=True, help="Enable cache hit/miss test")
    parser.add_argument("--similarity_threshold", type=float, default=0.7, 
                       help="Cosine similarity threshold for cache hit (0-1)")
    parser.add_argument("--max_cache_size", type=int, default=100, 
                       help="Maximum number of cached tasks")
    parser.add_argument("--num_hyperplanes", type=int, default=16, 
                       help="Number of hyperplanes for LSH")
    parser.add_argument("--fromdataset", type=str, 
                       default='/home/homie/homie/fuzzy_llama_submit/datasets/wiki_for_test.json',
                       help="Path to dataset JSON file")
    parser.add_argument("--max_count", type=int, default=10, 
                       help="Maximum number of samples to process")
    parser.add_argument("--model_path", type=str,
                       default="/mnt/sdb/homie/models/LLM-Research/Meta-Llama-3-8B-Instruct",
                       help="Path to pretrained model")
    return parser.parse_args()

try:
    # å¯¼å…¥Inter-Task KV Reuseæ¨¡å‹å®ç°
    from models.modeling_llama_inter_task_kv import LlamaForCausalLMWithKVReuse
    from models.inter_task_kv_manager import InterTaskKVManager
    print("âœ… æˆåŠŸåŠ è½½Inter-Task KV Reuseæ¨¡å‹ä»£ç ï¼")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯ï¼š{e}")
    print("è¯·ç¡®è®¤ï¼š")
    print("1. å½“å‰ç›®å½•å­˜åœ¨ models/modeling_llama_inter_task_kv.py")
    print("2. å½“å‰ç›®å½•å­˜åœ¨ models/inter_task_kv_manager.py")
    sys.exit(1)

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ç¯å¢ƒå˜é‡é…ç½®
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["OMP_NUM_THREADS"] = '1'

dir_path = Path("/mnt/sda1/homie_cache/")

# æ¸…ç†æ˜¾å­˜
def garbage_collection():
    gc.collect()
    torch.cuda.empty_cache()


# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
def start_model(model_path, similarity_threshold, max_cache_size, num_hyperplanes):
    print(f"\n{'='*60}")
    print("Loading model...")
    print(f"Model path: {model_path}")
    print(f"Similarity threshold: {similarity_threshold}")
    print(f"Max cache size: {max_cache_size}")
    print(f"Num hyperplanes: {num_hyperplanes}")
    print(f"{'='*60}\n")
    
    model = LlamaForCausalLMWithKVReuse.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
    ).to(device)
    
    # åˆ›å»ºKV Manager
    kv_manager = InterTaskKVManager(
        embedding_dim=model.config.hidden_size,
        max_cache_size=max_cache_size,
        similarity_threshold=similarity_threshold,
        num_hyperplanes=num_hyperplanes,
        device=str(device)
    )
    
    # è¿æ¥KV Manageråˆ°æ¨¡å‹
    model.set_kv_manager(kv_manager)
    
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print(f"âœ… Model loaded successfully!")
    print(f"Hidden size: {model.config.hidden_size}")
    print(f"Num layers: {model.config.num_hidden_layers}")
    print(f"Pad token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})")
    
    return model, tokenizer, kv_manager

# åœæ­¢æ¨¡å‹çš„å‡½æ•°
def stop_model(model):
    # åˆ é™¤æ¨¡å‹å¯¹è±¡
    del model
    garbage_collection()
    print("æ¨¡å‹å·²é‡Šæ”¾ï¼Œæ˜¾å­˜å·²æ¸…ç†")

def clean_cache(dir_path):
    if dir_path.exists() and dir_path.is_dir():
        shutil.rmtree(dir_path)  # åˆ é™¤ç›®å½•åŠå…¶å†…å®¹
        print(f"{dir_path} and its contents have been removed.")
    else:
        print(f"{dir_path} does not exist.")


def run_inference_with_kv_reuse(input_ids, attention_mask, model, tokenizer, task_id, kv_manager):
    """
    Run inference with KV reuse enabled
    """
    print(f"\n{'='*60}")
    print(f"Running inference for task: {task_id}")
    print(f"Input length: {input_ids.shape[-1]} tokens")
    print(f"{'='*60}")
    
    # è®¾ç½®ç»ˆæ­¢ç¬¦
    terminators = [tokenizer.eos_token_id]
    if tokenizer.pad_token_id is not None:
        terminators.append(tokenizer.pad_token_id)
    
    # é¦–å…ˆè®¡ç®—embeddingç”¨äºç›¸ä¼¼åº¦æœç´¢
    with torch.no_grad():
        inputs_embeds = model.model.embed_tokens(input_ids)
        task_embedding = kv_manager._compute_task_embedding(inputs_embeds)
    
    # æœç´¢ç›¸ä¼¼ä»»åŠ¡
    matched_entry = kv_manager.search_similar_task(task_embedding)
    
    start = time.time()
    
    if matched_entry is not None:
        print(f"ğŸ¯ Using cached KV from task: {matched_entry.task_id}")
        cache_hit = True
    else:
        print(f"ğŸ“ No cache hit, running full inference...")
        cache_hit = False
    
    # è¿è¡Œç”Ÿæˆ - ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆé¿å…è­¦å‘Š
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_new_tokens=10,
            eos_token_id=terminators,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=False,
            temperature=1.0,  # æ˜¾å¼è®¾ç½®é¿å…è­¦å‘Š
            top_p=1.0,        # æ˜¾å¼è®¾ç½®é¿å…è­¦å‘Š
            use_cache=True,
            return_dict_in_generate=True,
            output_hidden_states=False,
        )
    
    total_latency = time.time() - start
    
    # å¦‚æœæ˜¯cache missï¼Œä¿å­˜KVåˆ°ç¼“å­˜
    if not cache_hit and hasattr(outputs, 'past_key_values') and outputs.past_key_values is not None:
        last_layer_kv = outputs.past_key_values[-1]
        if last_layer_kv is not None:
            kv_manager.add_task(
                task_id=task_id,
                task_embedding=task_embedding,
                top_layer_kv=last_layer_kv
            )
    
    # è§£ç å“åº”
    if hasattr(outputs, 'sequences'):
        response_ids = outputs.sequences[0][input_ids.shape[-1]:]
    else:
        response_ids = outputs[0][input_ids.shape[-1]:]
    
    response = tokenizer.decode(response_ids, skip_special_tokens=True)
    
    print(f"ğŸ¤– Generated Answer: {response}")
    print(f"â±ï¸ Latency: {total_latency:.3f}s")
    print(f"{'='*60}\n")
    
    return total_latency, response, cache_hit


def test_cache_functionality(model, tokenizer, kv_manager, args):
    """
    Test cache hit/miss functionality with similar prompts
    ä½¿ç”¨ç®€çŸ­ç›¸ä¼¼çš„å¥å­è¿›è¡Œæµ‹è¯•
    """
    print("\n" + "="*80)
    print("CACHE FUNCTIONALITY TEST")
    print("Testing with short similar sentences")
    print("="*80)
    
    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼šä½¿ç”¨ç®€çŸ­ç›¸ä¼¼çš„å¥å­
    test_cases = [
        # ç¬¬ä¸€ç»„ï¼šå…³äºäººå·¥æ™ºèƒ½çš„ä¸­æ–‡å¥å­ï¼ˆé«˜åº¦ç›¸ä¼¼ï¼‰
        {
            "prompts": [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
                "äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆ",
                "è§£é‡Šäººå·¥æ™ºèƒ½çš„å«ä¹‰",
                "äººå·¥æ™ºèƒ½çš„å®šä¹‰æ˜¯ä»€ä¹ˆ",
                "ç»™æˆ‘è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            ],
            "group": "AI questions (Chinese)"
        },
        # ç¬¬äºŒç»„ï¼šå…³äºæœºå™¨å­¦ä¹ çš„è‹±æ–‡å¥å­
        {
            "prompts": [
                "What is machine learning?",
                "Explain machine learning.",
                "Define machine learning.",
                "What does machine learning mean?",
            ],
            "group": "ML questions (English)"
        },
        # ç¬¬ä¸‰ç»„ï¼šå…³äºçŒ«çš„ç®€çŸ­å¥å­
        {
            "prompts": [
                "The cat is sleeping.",
                "The cat is eating.",
                "The cat is playing.",
                "A cat is sleeping.",
            ],
            "group": "Cat sentences"
        },
        # ç¬¬å››ç»„ï¼šå®Œå…¨ä¸åŒçš„å¥å­
        {
            "prompts": [
                "Hello world!",
                "Good morning!",
                "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
            ],
            "group": "Unrelated sentences"
        },
    ]
    
    results = []
    total_hits = 0
    total_queries = 0
    
    for group_idx, test_group in enumerate(test_cases):
        print(f"\n{'='*60}")
        print(f"Testing Group {group_idx + 1}: {test_group['group']}")
        print(f"{'='*60}")
        
        for prompt_idx, prompt in enumerate(test_group['prompts']):
            total_queries += 1
            
            # ç›´æ¥tokenizeï¼Œä¸ä½¿ç”¨chat template
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512,
                return_attention_mask=True,
            )
            
            input_ids = inputs["input_ids"].to(model.device)
            attention_mask = inputs["attention_mask"].to(model.device)
            
            # ç”Ÿæˆtask_id
            task_id = f"group{group_idx}_prompt{prompt_idx}"
            
            # è¿è¡Œæ¨ç†
            latency, response, cache_hit = run_inference_with_kv_reuse(
                input_ids, attention_mask, model, tokenizer, task_id, kv_manager
            )
            
            if cache_hit:
                total_hits += 1
            
            results.append({
                "group": test_group['group'],
                "prompt": prompt,
                "latency": latency,
                "cache_hit": cache_hit,
                "response": response[:50] + "..." if len(response) > 50 else response
            })
            
            # æ¸…ç†æ˜¾å­˜
            garbage_collection()
            time.sleep(0.5)
    
    # æ‰“å°æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("TEST RESULTS SUMMARY")
    print("="*80)
    
    print(f"\n{'Prompt':<30} {'Cache Hit':<12} {'Latency':<10}")
    print("-"*52)
    for r in results:
        hit_str = "âœ… HIT" if r['cache_hit'] else "âŒ MISS"
        print(f"{r['prompt'][:28]:<30} {hit_str:<12} {r['latency']:.3f}s")
    
    print("\n" + "-"*52)
    hit_rate = total_hits / total_queries if total_queries > 0 else 0
    print(f"Total Queries: {total_queries}")
    print(f"Cache Hits: {total_hits}")
    print(f"Cache Misses: {total_queries - total_hits}")
    print(f"Hit Rate: {hit_rate:.2%}")
    
    # æ‰“å°KV Managerç»Ÿè®¡
    stats = kv_manager.get_statistics()
    print(f"\nKV Manager Statistics:")
    print(f"  Cache Size: {stats['cache_size']}")
    print(f"  Num Buckets: {stats['num_buckets']}")
    print(f"  Total Queries: {stats['total_queries']}")
    print(f"  Cache Hits: {stats['cache_hits']}")
    print(f"  Cache Misses: {stats['cache_misses']}")
    print(f"  Hit Rate: {stats['hit_rate']:.2%}")
    
    return results


def main():
    args = parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°
    
    print("\n" + "="*80)
    print("INTER-TASK KV REUSE TEST")
    print("="*80)
    print(f"Model: {args.model_path}")
    print(f"Similarity Threshold: {args.similarity_threshold}")
    print(f"Max Cache Size: {args.max_cache_size}")
    print(f"Num Hyperplanes: {args.num_hyperplanes}")
    print("="*80 + "\n")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, kv_manager = start_model(
        args.model_path, 
        args.similarity_threshold, 
        args.max_cache_size, 
        args.num_hyperplanes
    )
    
    time.sleep(2)
    
    # è¿è¡Œç¼“å­˜æµ‹è¯•
    if args.cache_test:
        results = test_cache_functionality(model, tokenizer, kv_manager, args)
    
    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æ·»åŠ TTFTæµ‹è¯•å’ŒF1æµ‹è¯•
    if args.ttft_test:
        print("\nTTFT test not implemented in this version")
    
    if args.f1_test:
        print("\nF1 test not implemented in this version")
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("TEST COMPLETE")
    print("="*80)
    
    # åœæ­¢æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜
    stop_model(model)
    clean_cache(dir_path)


if __name__ == "__main__":
    main()

‚úÖ ÊàêÂäüÂä†ËΩΩInter-Task KV ReuseÊ®°Âûã‰ª£Á†ÅÔºÅ
Using device: cuda

================================================================================
INTER-TASK KV REUSE TEST
================================================================================
Model: /mnt/sdb/homie/models/LLM-Research/Meta-Llama-3-8B-Instruct
Similarity Threshold: 0.7
Max Cache Size: 100
Num Hyperplanes: 16
================================================================================


============================================================
Loading model...
Model path: /mnt/sdb/homie/models/LLM-Research/Meta-Llama-3-8B-Instruct
Similarity threshold: 0.7
Max cache size: 100
Num hyperplanes: 16
============================================================

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  2.31it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.44it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.51it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.37it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.96it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
[LlamaModel] KV Reuse enabled
‚úÖ Model loaded successfully!
Hidden size: 4096
Num layers: 32
Pad token: <|eot_id|> (id=128009)

================================================================================
RUNNING DIAGNOSTIC TEST FIRST
================================================================================

================================================================================
DIAGNOSTIC RUN
================================================================================
Initial cache size: 0

--- Diagnostic prompt 1/3: '‰ªÄ‰πàÊòØ‰∫∫Â∑•Êô∫ËÉΩ' ---

======================================================================
Running inference for task: diag_0
Input length: 6 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=diag_0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 0, Buckets: 0
[KVManager] Bucket candidates: [], Total cached entries: 0
[KVManager] ‚ùå Cache MISS - No candidates available
======================================================================

üìù Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = diag_0
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=6, task_id=diag_0, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 6, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=diag_0, kv_present=True, key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2505
[KVManager] Penultimate hidden states shape: torch.Size([1, 6, 4096])
[KVManager] LSH hash: 110001101101...
[KVManager] ‚úÖ Task added. Cache size: 1. Buckets: 1
======================================================================

[LlamaModel] ‚úÖ Auto-saved KV for task diag_0 (kv_seq_len=6, input_seq_len=6)
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 6
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 7
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] ‚úÖ KV auto-saved for task diag_0

======================================================================
[KVManager] REQUEST: task_id=verify_diag_0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: ['diag_0'], Total cached entries: 1
[KVManager]   Candidate diag_0 sim=1.0000 norm=0.2505
[KVManager] ‚úÖ Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: diag_0
======================================================================

[Verify] ‚úÖ Task diag_0 successfully found in cache (matched: diag_0)

ü§ñ Generated Answer: ÔºàAIÔºâÔºü
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâ
‚è±Ô∏è Latency: 1.091s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 1
Number of buckets: 1
Total queries: 2
Cache hits: 1
Cache misses: 1
Hit rate: 50.00%

Buckets:
  110001101101...: 1 entries

Top 5 entries:
  [1] task_id=diag_0, timestamp=1, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


--- Diagnostic prompt 2/3: '‰∫∫Â∑•Êô∫ËÉΩÊòØ‰ªÄ‰πà' ---

======================================================================
Running inference for task: diag_1
Input length: 5 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=diag_1, query_hash=100011101100..., query_norm=0.2725
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: [], Total cached entries: 1
[KVManager]   Candidate diag_0 sim=0.8119 norm=0.2505
[KVManager] ‚úÖ Cache HIT! Similarity: 0.8119 >= 0.7
[KVManager] Matched task: diag_0
======================================================================

üéØ Cache HIT! Using cached KV from task: diag_0
üöÄ LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 5
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
Traceback (most recent call last):
  File "/home/homie/SimLLM/inter_task_kv_test.py", line 728, in <module>
    main()
  File "/home/homie/SimLLM/inter_task_kv_test.py", line 698, in main
    diag_success = diagnostic_run(diagnostic_prompts, model, tokenizer, kv_manager)
  File "/home/homie/SimLLM/inter_task_kv_test.py", line 491, in diagnostic_run
    latency, response, cache_hit, add_success = run_inference_with_kv_reuse(
  File "/home/homie/SimLLM/inter_task_kv_test.py", line 325, in run_inference_with_kv_reuse
    outputs = model(
  File "/home/homie/.conda/envs/simllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/homie/.conda/envs/simllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/homie/SimLLM/models/modeling_llama_inter_task_kv.py", line 761, in forward
    outputs = self.model(
  File "/home/homie/.conda/envs/simllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/homie/.conda/envs/simllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/homie/SimLLM/models/modeling_llama_inter_task_kv.py", line 268, in forward
    raise ValueError("You have to specify either input_ids or inputs_embeds")
ValueError: You have to specify either input_ids or inputs_embeds

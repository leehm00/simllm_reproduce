yes ÊàêÂäüÂä†ËΩΩInter-Task KV ReuseÊ®°Âûã‰ª£Á†ÅÔºÅ
Using device: cuda

================================================================================
INTER-TASK KV REUSE TEST
================================================================================
Model: /mnt/sdb/homie/models/LLM-Research/Meta-Llama-3-8B-Instruct
Similarity Threshold: 0.7
Max Cache Size: 100
Num Hyperplanes: 16
================================================================================


============================================================
Loading model...
Model path: /mnt/sdb/homie/models/LLM-Research/Meta-Llama-3-8B-Instruct
Similarity threshold: 0.7
Max cache size: 100
Num hyperplanes: 16
============================================================

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  2.40it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.50it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.60it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.48it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.05it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
[LlamaModel] KV Reuse enabled
yes Model loaded successfully!
Hidden size: 4096
Num layers: 32
Pad token: <|eot_id|> (id=128009)

================================================================================
RUNNING DIAGNOSTIC TEST FIRST
================================================================================

================================================================================
DIAGNOSTIC RUN
================================================================================
Initial cache size: 0

--- Diagnostic prompt 1/3: '‰ªÄ‰πàÊòØ‰∫∫Â∑•Êô∫ËÉΩ' ---

======================================================================
Running inference for task: diag_0
Input length: 6 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=diag_0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 0, Buckets: 0
[KVManager] Bucket candidates: [], Total cached entries: 0
[KVManager]  Cache MISS - No candidates available
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = diag_0
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=6, task_id=diag_0, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 6, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=diag_0, kv_present=True, key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2505
[KVManager] Penultimate hidden states shape: torch.Size([1, 6, 4096])
[KVManager] LSH hash: 110001101101...
[KVManager]  Task added. Cache size: 1. Buckets: 1
======================================================================

[LlamaModel]  Auto-saved KV for task diag_0 (kv_seq_len=6, input_seq_len=6)
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 6
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 7
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_0
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task diag_0

======================================================================
[KVManager] REQUEST: task_id=verify_diag_0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: ['diag_0'], Total cached entries: 1
[KVManager]   Candidate diag_0 sim=1.0000 norm=0.2505
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: diag_0
======================================================================

[Verify] yes Task diag_0 successfully found in cache (matched: diag_0)

ü§ñ Generated Answer: ÔºàAIÔºâÔºü
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠ÁöÑ‰∏ÄÁßçÁ†îÁ©∂È¢ÜÂüüÔºåÊó®Âú®ÂºÄÂèëËÉΩÂ§üÊ®°Êãü„ÄÅÊâ©Â±ïÂíåË∂ÖË∂ä‰∫∫Á±ªÊô∫ËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇAIÁ≥ªÁªüÂèØ‰ª•Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥
‚è±Ô∏è Latency: 2.602s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 1
Number of buckets: 1
Total queries: 2
Cache hits: 1
Cache misses: 1
Hit rate: 50.00%

Buckets:
  110001101101...: 1 entries

Top 5 entries:
  [1] task_id=diag_0, timestamp=1, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


--- Diagnostic prompt 2/3: '‰∫∫Â∑•Êô∫ËÉΩÊòØ‰ªÄ‰πà' ---

======================================================================
Running inference for task: diag_1
Input length: 5 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=diag_1, query_hash=100011101100..., query_norm=0.2725
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: [], Total cached entries: 1
[KVManager]   Candidate diag_0 sim=0.8119 norm=0.2505
[KVManager]  Cache HIT! Similarity: 0.8119 >= 0.7
[KVManager] Matched task: diag_0
======================================================================

 Cache HIT! Using cached KV from task: diag_0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 5
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.831s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 1
Number of buckets: 1
Total queries: 3
Cache hits: 2
Cache misses: 1
Hit rate: 66.67%

Buckets:
  110001101101...: 1 entries

Top 5 entries:
  [1] task_id=diag_0, timestamp=2, access_count=3
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


--- Diagnostic prompt 3/3: 'Ëß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑÂê´‰πâ' ---

======================================================================
Running inference for task: diag_2
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=diag_2, query_hash=100001101000..., query_norm=0.2247
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: [], Total cached entries: 1
[KVManager]   Candidate diag_0 sim=0.6396 norm=0.2505
[KVManager]  Cache MISS. Best similarity: 0.6396 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = diag_2
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=9, task_id=diag_2, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 9, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=diag_2, kv_present=True, key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2247
[KVManager] Penultimate hidden states shape: torch.Size([1, 9, 4096])
[KVManager] LSH hash: 100001101000...
[KVManager]  Task added. Cache size: 2. Buckets: 2
======================================================================

[LlamaModel]  Auto-saved KV for task diag_2 (kv_seq_len=9, input_seq_len=9)
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 55
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 56
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task diag_2
[LlamaModel] Using DynamicCache, initial length: 57
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task diag_2

======================================================================
[KVManager] REQUEST: task_id=verify_diag_2, query_hash=100001101000..., query_norm=0.2247
[KVManager] Cache size: 2, Buckets: 2
[KVManager] Bucket candidates: ['diag_2'], Total cached entries: 2
[KVManager]   Candidate diag_2 sim=1.0000 norm=0.2247
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: diag_2
======================================================================

[Verify] yes Task diag_2 successfully found in cache (matched: diag_2)

ü§ñ Generated Answer: 
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàê„ÄÅËßÜËßâËØÜÂà´Á≠â„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ
‚è±Ô∏è Latency: 1.975s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 2
Number of buckets: 2
Total queries: 5
Cache hits: 3
Cache misses: 2
Hit rate: 60.00%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries

Top 5 entries:
  [1] task_id=diag_0, timestamp=2, access_count=3
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [2] task_id=diag_2, timestamp=4, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
======================================================================

======================================================================


Final cache size: 2

Diagnostic Summary:
  Prompts processed: 3
  Cache hits: 1
  Cache misses: 2
  Successful saves: 2
  Failed saves: 0
  Cache size increase: 2

yes ASSERTION PASSED: Cache is working correctly
   - 2/2 cache misses were saved successfully
   - Final cache size: 2
[KVManager] Cache reset

================================================================================
CACHE FUNCTIONALITY TEST
Testing with short similar sentences
================================================================================

============================================================
Testing Group 1: AI questions (Chinese) - Primary Test
============================================================

======================================================================
Running inference for task: group0_prompt0
Input length: 6 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 0, Buckets: 0
[KVManager] Bucket candidates: [], Total cached entries: 0
[KVManager]  Cache MISS - No candidates available
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group0_prompt0
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=6, task_id=group0_prompt0, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 6, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group0_prompt0, kv_present=True, key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2505
[KVManager] Penultimate hidden states shape: torch.Size([1, 6, 4096])
[KVManager] LSH hash: 110001101101...
[KVManager]  Task added. Cache size: 1. Buckets: 1
======================================================================

[LlamaModel]  Auto-saved KV for task group0_prompt0 (kv_seq_len=6, input_seq_len=6)
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 6
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 7
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt0
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group0_prompt0

======================================================================
[KVManager] REQUEST: task_id=verify_group0_prompt0, query_hash=110001101101..., query_norm=0.2505
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: ['group0_prompt0'], Total cached entries: 1
[KVManager]   Candidate group0_prompt0 sim=1.0000 norm=0.2505
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group0_prompt0
======================================================================

[Verify] yes Task group0_prompt0 successfully found in cache (matched: group0_prompt0)

ü§ñ Generated Answer: ÔºàAIÔºâÔºü
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠ÁöÑ‰∏ÄÁßçÁ†îÁ©∂È¢ÜÂüüÔºåÊó®Âú®ÂºÄÂèëËÉΩÂ§üÊ®°Êãü„ÄÅÊâ©Â±ïÂíåË∂ÖË∂ä‰∫∫Á±ªÊô∫ËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇAIÁ≥ªÁªüÂèØ‰ª•Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥
‚è±Ô∏è Latency: 1.966s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 1
Number of buckets: 1
Total queries: 2
Cache hits: 1
Cache misses: 1
Hit rate: 50.00%

Buckets:
  110001101101...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt0, timestamp=1, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt1
Input length: 5 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt1, query_hash=100011101100..., query_norm=0.2725
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: [], Total cached entries: 1
[KVManager]   Candidate group0_prompt0 sim=0.8119 norm=0.2505
[KVManager]  Cache HIT! Similarity: 0.8119 >= 0.7
[KVManager] Matched task: group0_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 5
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.830s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 1
Number of buckets: 1
Total queries: 3
Cache hits: 2
Cache misses: 1
Hit rate: 66.67%

Buckets:
  110001101101...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt0, timestamp=2, access_count=3
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt2
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt2, query_hash=100001101000..., query_norm=0.2247
[KVManager] Cache size: 1, Buckets: 1
[KVManager] Bucket candidates: [], Total cached entries: 1
[KVManager]   Candidate group0_prompt0 sim=0.6396 norm=0.2505
[KVManager]  Cache MISS. Best similarity: 0.6396 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group0_prompt2
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=9, task_id=group0_prompt2, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 9, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group0_prompt2, kv_present=True, key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2247
[KVManager] Penultimate hidden states shape: torch.Size([1, 9, 4096])
[KVManager] LSH hash: 100001101000...
[KVManager]  Task added. Cache size: 2. Buckets: 2
======================================================================

[LlamaModel]  Auto-saved KV for task group0_prompt2 (kv_seq_len=9, input_seq_len=9)
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 55
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 56
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt2
[LlamaModel] Using DynamicCache, initial length: 57
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group0_prompt2

======================================================================
[KVManager] REQUEST: task_id=verify_group0_prompt2, query_hash=100001101000..., query_norm=0.2247
[KVManager] Cache size: 2, Buckets: 2
[KVManager] Bucket candidates: ['group0_prompt2'], Total cached entries: 2
[KVManager]   Candidate group0_prompt2 sim=1.0000 norm=0.2247
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group0_prompt2
======================================================================

[Verify] yes Task group0_prompt2 successfully found in cache (matched: group0_prompt2)

ü§ñ Generated Answer: 
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàê„ÄÅËßÜËßâËØÜÂà´Á≠â„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ
‚è±Ô∏è Latency: 1.967s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 2
Number of buckets: 2
Total queries: 5
Cache hits: 3
Cache misses: 2
Hit rate: 60.00%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt0, timestamp=2, access_count=3
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [2] task_id=group0_prompt2, timestamp=4, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt3
Input length: 7 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt3, query_hash=010001100100..., query_norm=0.2411
[KVManager] Cache size: 2, Buckets: 2
[KVManager] Bucket candidates: [], Total cached entries: 2
[KVManager]   Candidate group0_prompt0 sim=0.7576 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.6752 norm=0.2247
[KVManager]  Cache HIT! Similarity: 0.7576 >= 0.7
[KVManager] Matched task: group0_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 7
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.832s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 2
Number of buckets: 2
Total queries: 6
Cache hits: 4
Cache misses: 2
Hit rate: 66.67%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=4, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt0, timestamp=5, access_count=4
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt4
Input length: 8 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt4, query_hash=011001101100..., query_norm=0.2339
[KVManager] Cache size: 2, Buckets: 2
[KVManager] Bucket candidates: [], Total cached entries: 2
[KVManager]   Candidate group0_prompt0 sim=0.6806 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.6441 norm=0.2247
[KVManager]  Cache MISS. Best similarity: 0.6806 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group0_prompt4
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=8, task_id=group0_prompt4, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 8, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group0_prompt4, kv_present=True, key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2339
[KVManager] Penultimate hidden states shape: torch.Size([1, 8, 4096])
[KVManager] LSH hash: 011001101100...
[KVManager]  Task added. Cache size: 3. Buckets: 3
======================================================================

[LlamaModel]  Auto-saved KV for task group0_prompt4 (kv_seq_len=8, input_seq_len=8)
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 55
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt4
[LlamaModel] Using DynamicCache, initial length: 56
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group0_prompt4

======================================================================
[KVManager] REQUEST: task_id=verify_group0_prompt4, query_hash=011001101100..., query_norm=0.2339
[KVManager] Cache size: 3, Buckets: 3
[KVManager] Bucket candidates: ['group0_prompt4'], Total cached entries: 3
[KVManager]   Candidate group0_prompt4 sim=1.0000 norm=0.2339
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group0_prompt4
======================================================================

[Verify] yes Task group0_prompt4 successfully found in cache (matched: group0_prompt4)

ü§ñ Generated Answer: „ÄÅÂèëÂ±ïÂéÜÂè≤ÂíåÂ∫îÁî®Âú∫ÊôØ
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢òÂíåÊÑüÁü•ÁéØÂ¢ÉÁ≠â„ÄÇ‰∫∫Â∑•
‚è±Ô∏è Latency: 1.959s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 3
Number of buckets: 3
Total queries: 8
Cache hits: 5
Cache misses: 3
Hit rate: 62.50%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=4, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt0, timestamp=5, access_count=4
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [3] task_id=group0_prompt4, timestamp=7, access_count=2
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt5
Input length: 10 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt5, query_hash=010011101000..., query_norm=0.2192
[KVManager] Cache size: 3, Buckets: 3
[KVManager] Bucket candidates: [], Total cached entries: 3
[KVManager]   Candidate group0_prompt0 sim=0.6290 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.7316 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.5819 norm=0.2339
[KVManager]  Cache HIT! Similarity: 0.7316 >= 0.7
[KVManager] Matched task: group0_prompt2
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt2
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 9, Input seq_len: 10
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 9, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 9, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 9, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 9, 128])
[LlamaModel] KV Reuse: Q_len=9, KV_len=9
[LlamaAttention] KV Reuse Mode: q_len=9, past_kv_len=9
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: 
        in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.829s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 3
Number of buckets: 3
Total queries: 9
Cache hits: 6
Cache misses: 3
Hit rate: 66.67%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt0, timestamp=5, access_count=4
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [2] task_id=group0_prompt4, timestamp=7, access_count=2
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt6
Input length: 8 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt6, query_hash=000111101100..., query_norm=0.2356
[KVManager] Cache size: 3, Buckets: 3
[KVManager] Bucket candidates: [], Total cached entries: 3
[KVManager]   Candidate group0_prompt0 sim=0.7983 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.5919 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.6088 norm=0.2339
[KVManager]  Cache HIT! Similarity: 0.7983 >= 0.7
[KVManager] Matched task: group0_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 8
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.827s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 3
Number of buckets: 3
Total queries: 10
Cache hits: 7
Cache misses: 3
Hit rate: 70.00%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt4, timestamp=7, access_count=2
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [2] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [3] task_id=group0_prompt0, timestamp=9, access_count=5
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt7
Input length: 8 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt7, query_hash=100011101100..., query_norm=0.2333
[KVManager] Cache size: 3, Buckets: 3
[KVManager] Bucket candidates: [], Total cached entries: 3
[KVManager]   Candidate group0_prompt0 sim=0.6824 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.6577 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.7965 norm=0.2339
[KVManager]  Cache HIT! Similarity: 0.7965 >= 0.7
[KVManager] Matched task: group0_prompt4
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt4
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 8, Input seq_len: 8
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 8, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 8, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 8, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 8, 128])
[LlamaModel] KV Reuse: Q_len=8, KV_len=8
[LlamaAttention] KV Reuse Mode: q_len=8, past_kv_len=8
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: „ÄÅ BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.831s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 3
Number of buckets: 3
Total queries: 11
Cache hits: 8
Cache misses: 3
Hit rate: 72.73%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt0, timestamp=9, access_count=5
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [3] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt8
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt8, query_hash=010001101100..., query_norm=0.2229
[KVManager] Cache size: 3, Buckets: 3
[KVManager] Bucket candidates: [], Total cached entries: 3
[KVManager]   Candidate group0_prompt0 sim=0.6770 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.5588 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.5973 norm=0.2339
[KVManager]  Cache MISS. Best similarity: 0.6770 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group0_prompt8
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=9, task_id=group0_prompt8, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 9, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group0_prompt8, kv_present=True, key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2229
[KVManager] Penultimate hidden states shape: torch.Size([1, 9, 4096])
[KVManager] LSH hash: 010001101100...
[KVManager]  Task added. Cache size: 4. Buckets: 4
======================================================================

[LlamaModel]  Auto-saved KV for task group0_prompt8 (kv_seq_len=9, input_seq_len=9)
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 55
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 56
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group0_prompt8
[LlamaModel] Using DynamicCache, initial length: 57
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 58, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group0_prompt8

======================================================================
[KVManager] REQUEST: task_id=verify_group0_prompt8, query_hash=010001101100..., query_norm=0.2229
[KVManager] Cache size: 4, Buckets: 4
[KVManager] Bucket candidates: ['group0_prompt8'], Total cached entries: 4
[KVManager]   Candidate group0_prompt8 sim=1.0000 norm=0.2229
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group0_prompt8
======================================================================

[Verify] yes Task group0_prompt8 successfully found in cache (matched: group0_prompt8)

ü§ñ Generated Answer:  - AI Applications
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØ‰∏ÄÁßçÊ®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑÊäÄÊúØÔºåÂÆÉÂèØ‰ª•Ëá™Âä®Â≠¶‰π†„ÄÅÊé®ÁêÜÂíåËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇAIÊäÄÊúØÊúâÂæàÂ§öÂ∫îÁî®È¢ÜÂüüÔºå‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂ∏∏ËßÅÁöÑÂ∫îÁî®Ôºö

1.
‚è±Ô∏è Latency: 1.967s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 4
Number of buckets: 4
Total queries: 13
Cache hits: 9
Cache misses: 4
Hit rate: 69.23%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt0, timestamp=9, access_count=5
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [3] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [4] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group0_prompt9
Input length: 7 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group0_prompt9, query_hash=000001100100..., query_norm=0.2417
[KVManager] Cache size: 4, Buckets: 4
[KVManager] Bucket candidates: [], Total cached entries: 4
[KVManager]   Candidate group0_prompt0 sim=0.7431 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.6477 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.6936 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.7324 norm=0.2229
[KVManager]  Cache HIT! Similarity: 0.7431 >= 0.7
[KVManager] Matched task: group0_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group0_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 7
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
‚è±Ô∏è Latency: 1.831s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 4
Number of buckets: 4
Total queries: 14
Cache hits: 10
Cache misses: 4
Hit rate: 71.43%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


============================================================
Testing Group 2: ML questions (English)
============================================================

======================================================================
Running inference for task: group1_prompt0
Input length: 6 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt0, query_hash=111110110101..., query_norm=0.2085
[KVManager] Cache size: 4, Buckets: 4
[KVManager] Bucket candidates: [], Total cached entries: 4
[KVManager]   Candidate group0_prompt0 sim=0.3042 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2267 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2806 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.3061 norm=0.2229
[KVManager]  Cache MISS. Best similarity: 0.3061 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group1_prompt0
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=6, task_id=group1_prompt0, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 6, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group1_prompt0, kv_present=True, key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2085
[KVManager] Penultimate hidden states shape: torch.Size([1, 6, 4096])
[KVManager] LSH hash: 111110110101...
[KVManager]  Task added. Cache size: 5. Buckets: 5
======================================================================

[LlamaModel]  Auto-saved KV for task group1_prompt0 (kv_seq_len=6, input_seq_len=6)
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 6
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 7
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt0
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group1_prompt0

======================================================================
[KVManager] REQUEST: task_id=verify_group1_prompt0, query_hash=111110110101..., query_norm=0.2085
[KVManager] Cache size: 5, Buckets: 5
[KVManager] Bucket candidates: ['group1_prompt0'], Total cached entries: 5
[KVManager]   Candidate group1_prompt0 sim=1.0000 norm=0.2085
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

[Verify] yes Task group1_prompt0 successfully found in cache (matched: group1_prompt0)

ü§ñ Generated Answer:  Machine learning is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It involves training algorithms on large datasets to recognize patterns, make predictions, and improve their performance over time.
Machine learning is a subset of
‚è±Ô∏è Latency: 1.965s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 5
Number of buckets: 5
Total queries: 16
Cache hits: 11
Cache misses: 5
Hit rate: 68.75%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt0, timestamp=15, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt1
Input length: 6 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt1, query_hash=011100110001..., query_norm=0.2123
[KVManager] Cache size: 5, Buckets: 5
[KVManager] Bucket candidates: [], Total cached entries: 5
[KVManager]   Candidate group0_prompt0 sim=0.2589 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2353 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2609 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2408 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.6886 norm=0.2085
[KVManager]  Cache MISS. Best similarity: 0.6886 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group1_prompt1
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=6, task_id=group1_prompt1, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 6, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 6, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group1_prompt1, kv_present=True, key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.2123
[KVManager] Penultimate hidden states shape: torch.Size([1, 6, 4096])
[KVManager] LSH hash: 011100110001...
[KVManager]  Task added. Cache size: 6. Buckets: 6
======================================================================

[LlamaModel]  Auto-saved KV for task group1_prompt1 (kv_seq_len=6, input_seq_len=6)
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 6
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 7, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 7
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt1
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group1_prompt1

======================================================================
[KVManager] REQUEST: task_id=verify_group1_prompt1, query_hash=011100110001..., query_norm=0.2123
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: ['group1_prompt1'], Total cached entries: 6
[KVManager]   Candidate group1_prompt1 sim=1.0000 norm=0.2123
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group1_prompt1
======================================================================

[Verify] yes Task group1_prompt1 successfully found in cache (matched: group1_prompt1)

ü§ñ Generated Answer:  What are the types of machine learning?
Machine learning is a subset of artificial intelligence (AI) that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed. The algorithms learn by identifying patterns and relationships in the data, and
‚è±Ô∏è Latency: 1.965s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 18
Cache hits: 12
Cache misses: 6
Hit rate: 66.67%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt0, timestamp=15, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt2
Input length: 5 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt2, query_hash=011111101001..., query_norm=0.2299
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: [], Total cached entries: 6
[KVManager]   Candidate group0_prompt0 sim=0.2864 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2300 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2760 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2479 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.7292 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.7129 norm=0.2123
[KVManager]  Cache HIT! Similarity: 0.7292 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 5
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.829s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 19
Cache hits: 13
Cache misses: 6
Hit rate: 68.42%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt3
Input length: 7 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt3, query_hash=011110101001..., query_norm=0.2064
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: [], Total cached entries: 6
[KVManager]   Candidate group0_prompt0 sim=0.2709 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2135 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2686 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2734 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.7792 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.6493 norm=0.2123
[KVManager]  Cache HIT! Similarity: 0.7792 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 7
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.828s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 20
Cache hits: 14
Cache misses: 6
Hit rate: 70.00%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt4
Input length: 7 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt4, query_hash=110100110100..., query_norm=0.2086
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: [], Total cached entries: 6
[KVManager]   Candidate group0_prompt0 sim=0.2824 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2209 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2691 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2797 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.8823 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.6390 norm=0.2123
[KVManager]  Cache HIT! Similarity: 0.8823 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 7
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.829s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 21
Cache hits: 15
Cache misses: 6
Hit rate: 71.43%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt5
Input length: 8 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt5, query_hash=011110110101..., query_norm=0.2029
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: [], Total cached entries: 6
[KVManager]   Candidate group0_prompt0 sim=0.2599 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2152 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2596 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2702 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.7916 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.6377 norm=0.2123
[KVManager]  Cache HIT! Similarity: 0.7916 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 8
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.830s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 22
Cache hits: 16
Cache misses: 6
Hit rate: 72.73%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt6
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt6, query_hash=111110110101..., query_norm=0.1859
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: ['group1_prompt0'], Total cached entries: 6
[KVManager]   Candidate group1_prompt0 sim=0.8706 norm=0.2085
[KVManager]  Cache HIT! Similarity: 0.8706 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 9
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.827s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 6
Number of buckets: 6
Total queries: 23
Cache hits: 17
Cache misses: 6
Hit rate: 73.91%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt7
Input length: 8 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt7, query_hash=011100010111..., query_norm=0.1940
[KVManager] Cache size: 6, Buckets: 6
[KVManager] Bucket candidates: [], Total cached entries: 6
[KVManager]   Candidate group0_prompt0 sim=0.2832 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2451 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.3037 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2614 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.6945 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.6562 norm=0.2123
[KVManager]  Cache MISS. Best similarity: 0.6945 < 0.7
======================================================================

 Cache MISS, running full inference (KV will be auto-saved)...
[Cache MISS] Set model.model.current_task_id = group1_prompt7
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Auto-save check: seq_len=8, task_id=group1_prompt7, use_cache=True
[LlamaModel] next_cache type: tuple
[LlamaModel] next_cache length: 32
[LlamaModel] next_cache[0] type: tuple
[LlamaModel] next_cache[0][0] (key) type: Tensor, shape: torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: tuple detected, len=32
[LlamaModel] _extract_last_layer_kv: next_cache[0] key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: next_cache[-1] key.shape=torch.Size([1, 8, 8, 128])
[LlamaModel] _extract_last_layer_kv: Found valid KV at layer 31, key.shape=torch.Size([1, 8, 8, 128])

======================================================================
[KVManager] ADD ATTEMPT: task_id=group1_prompt7, kv_present=True, key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
[KVManager] Embedding shape: torch.Size([4096]), norm=0.1940
[KVManager] Penultimate hidden states shape: torch.Size([1, 8, 4096])
[KVManager] LSH hash: 011100010111...
[KVManager]  Task added. Cache size: 7. Buckets: 7
======================================================================

[LlamaModel]  Auto-saved KV for task group1_prompt7 (kv_seq_len=8, input_seq_len=8)
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 8
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 9, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 9
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 10, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 10
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 11, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 11
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 12, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 12
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 13, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 13
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 14, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 14
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 15, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 15
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 16, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 16
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 17, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 17
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 18, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 18
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 19, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 19
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 20, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 20
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 21, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 21
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 22, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 22
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 23, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 23
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 24, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 24
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 25, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 25
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 26, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 26
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 27, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 27
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 28, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 28
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 29, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 29
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 30, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 30
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 31, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 31
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 32, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 32
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 33, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 33
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 34, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 34
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 35, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 35
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 36, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 36
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 37, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 37
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 38, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 38
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 39, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 39
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 40, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 40
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 41, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 41
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 42, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 42
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 43, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 43
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 44, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 44
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 45, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 45
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 46, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 46
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 47, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 47
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 48, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 48
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 49, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 49
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 50, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 50
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 51, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 51
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 52, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 52
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 53, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 53
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 54, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 54
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 55, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 55
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 56, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[LlamaModel] Forcing use_cache=True for task group1_prompt7
[LlamaModel] Using DynamicCache, initial length: 56
[LlamaModel] Layer 0 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Layer 31 KV: key.shape=torch.Size([1, 8, 57, 128])
[LlamaModel] Extracted 32 non-None KV pairs from DynamicCache
[Auto-Save] yes KV auto-saved for task group1_prompt7

======================================================================
[KVManager] REQUEST: task_id=verify_group1_prompt7, query_hash=011100010111..., query_norm=0.1940
[KVManager] Cache size: 7, Buckets: 7
[KVManager] Bucket candidates: ['group1_prompt7'], Total cached entries: 7
[KVManager]   Candidate group1_prompt7 sim=1.0000 norm=0.1940
[KVManager]  Cache HIT! Similarity: 1.0000 >= 0.7
[KVManager] Matched task: group1_prompt7
======================================================================

[Verify] yes Task group1_prompt7 successfully found in cache (matched: group1_prompt7)

ü§ñ Generated Answer:  How does it differ from traditional programming?
Machine learning is a subfield of artificial intelligence that involves training algorithms to learn from data, without being explicitly programmed. In traditional programming, a programmer writes code that specifies exactly what the program should do in every situation
‚è±Ô∏è Latency: 1.957s
üìä Cache Hit: False

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 7
Number of buckets: 7
Total queries: 25
Cache hits: 18
Cache misses: 7
Hit rate: 72.00%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries
  011100010111...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt8
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt8, query_hash=111110110101..., query_norm=0.1843
[KVManager] Cache size: 7, Buckets: 7
[KVManager] Bucket candidates: ['group1_prompt0'], Total cached entries: 7
[KVManager]   Candidate group1_prompt0 sim=0.8858 norm=0.2085
[KVManager]  Cache HIT! Similarity: 0.8858 >= 0.7
[KVManager] Matched task: group1_prompt0
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt0
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 6, Input seq_len: 9
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 6, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 6, 128])
[LlamaModel] KV Reuse: Q_len=6, KV_len=6
[LlamaAttention] KV Reuse Mode: q_len=6, past_kv_len=6
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.825s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 7
Number of buckets: 7
Total queries: 26
Cache hits: 19
Cache misses: 7
Hit rate: 73.08%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries
  011100010111...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


======================================================================
Running inference for task: group1_prompt9
Input length: 9 tokens
======================================================================

======================================================================
[KVManager] REQUEST: task_id=group1_prompt9, query_hash=011110100101..., query_norm=0.1914
[KVManager] Cache size: 7, Buckets: 7
[KVManager] Bucket candidates: [], Total cached entries: 7
[KVManager]   Candidate group0_prompt0 sim=0.2638 norm=0.2505
[KVManager]   Candidate group0_prompt2 sim=0.2498 norm=0.2247
[KVManager]   Candidate group0_prompt4 sim=0.2955 norm=0.2339
[KVManager]   Candidate group0_prompt8 sim=0.2460 norm=0.2229
[KVManager]   Candidate group1_prompt0 sim=0.6961 norm=0.2085
[KVManager]   Candidate group1_prompt1 sim=0.6556 norm=0.2123
[KVManager]   Candidate group1_prompt7 sim=0.7524 norm=0.1940
[KVManager]  Cache HIT! Similarity: 0.7524 >= 0.7
[KVManager] Matched task: group1_prompt7
======================================================================

 Cache HIT! Using cached KV from task: group1_prompt7
 LAYER SKIPPING MODE: Skipping layers 0 to N-2
[Cache HIT] Cached KV seq_len: 8, Input seq_len: 9
[Cache HIT] cached_penultimate_hidden.shape: torch.Size([1, 8, 4096])
[Cache HIT] last_layer_kv key.shape: torch.Size([1, 8, 8, 128])
[Cache HIT] Running forward with skip_to_last_layer=True...
[LlamaModel] Using DynamicCache, initial length: 0
[LlamaModel] üöÄ LAYER SKIPPING MODE: Using cached penultimate hidden states
[LlamaModel]    cached_penultimate_hidden.shape: torch.Size([1, 8, 4096])
[LlamaModel]    last_layer_kv key.shape: torch.Size([1, 8, 8, 128])
[LlamaModel] KV Reuse: Q_len=8, KV_len=8
[LlamaAttention] KV Reuse Mode: q_len=8, past_kv_len=8
[LlamaModel] Layer skipping complete: Only computed layer 31
[LlamaModel] Extracted 0 non-None KV pairs from DynamicCache
[Cache HIT]  Layer skipping complete, generated 50 tokens

ü§ñ Generated Answer:  How more:// Angeles States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
‚è±Ô∏è Latency: 1.825s
üìä Cache Hit: True

======================================================================
[KVManager] CACHE STATE DUMP
======================================================================
Cache size: 7
Number of buckets: 7
Total queries: 27
Cache hits: 20
Cache misses: 7
Hit rate: 74.07%

Buckets:
  110001101101...: 1 entries
  100001101000...: 1 entries
  011001101100...: 1 entries
  010001101100...: 1 entries
  111110110101...: 1 entries
  011100110001...: 1 entries
  011100010111...: 1 entries

Top 5 entries:
  [1] task_id=group0_prompt2, timestamp=8, access_count=3
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [2] task_id=group0_prompt4, timestamp=10, access_count=3
       key.shape=torch.Size([1, 8, 8, 128]), value.shape=torch.Size([1, 8, 8, 128])
  [3] task_id=group0_prompt8, timestamp=12, access_count=2
       key.shape=torch.Size([1, 8, 9, 128]), value.shape=torch.Size([1, 8, 9, 128])
  [4] task_id=group0_prompt0, timestamp=13, access_count=6
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
  [5] task_id=group1_prompt1, timestamp=17, access_count=2
       key.shape=torch.Size([1, 8, 6, 128]), value.shape=torch.Size([1, 8, 6, 128])
======================================================================

======================================================================


====================================================================================================
TEST RESULTS SUMMARY
====================================================================================================

#    Prompt                              Hit    Latency    Response (first 100 chars)                                                                          
-----------------------------------------------------------------------------------------------------------------------------------------------------------
1    ‰ªÄ‰πàÊòØ‰∫∫Â∑•Êô∫ËÉΩ                             MISS   1.966s    ÔºàAIÔºâÔºü ‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠ÁöÑ‰∏ÄÁßçÁ†îÁ©∂È¢ÜÂüüÔºåÊó®Âú®ÂºÄÂèëËÉΩÂ§üÊ®°Êãü„ÄÅÊâ©Â±ïÂíåË∂ÖË∂ä‰∫∫Á±ªÊô∫ËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇAIÁ≥ªÁªüÂèØ‰ª•Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥
2    ‰∫∫Â∑•Êô∫ËÉΩÊòØ‰ªÄ‰πà                             HIT    1.830s    Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
3    Ëß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑÂê´‰πâ                           MISS   1.967s     ‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàê„ÄÅËßÜËßâËØÜÂà´Á≠â„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ
4    ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆö‰πâÊòØ‰ªÄ‰πà                          HIT    1.832s    Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
5    ËØ∑ËØ¥Êòé‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ¶ÇÂøµ                          MISS   1.959s    „ÄÅÂèëÂ±ïÂéÜÂè≤ÂíåÂ∫îÁî®Âú∫ÊôØ ‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢òÂíåÊÑüÁü•ÁéØÂ¢ÉÁ≠â„ÄÇ‰∫∫Â∑•
6    ËÉΩÂê¶Ëß£Èáä‰∏Ä‰∏ã‰∫∫Â∑•Êô∫ËÉΩÔºü                         HIT    1.829s             in the States States States States States States States States States States States States ...
7    ‰∫∫Â∑•Êô∫ËÉΩÊåáÁöÑÊòØ‰ªÄ‰πàÔºü                          HIT    1.827s    Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
8    ËØ∑ÊèèËø∞‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰ΩúÁî®                          HIT    1.831s    „ÄÅ BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
9    ‰∫∫Â∑•Êô∫ËÉΩÊúâÂì™‰∫õÂ∫îÁî®Ôºü                          MISS   1.967s     - AI Applications ‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØ‰∏ÄÁßçÊ®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑÊäÄÊúØÔºåÂÆÉÂèØ‰ª•Ëá™Âä®Â≠¶‰π†„ÄÅÊé®ÁêÜÂíåËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇAIÊäÄÊúØÊúâÂæàÂ§öÂ∫îÁî®È¢ÜÂüüÔºå‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂ∏∏ËßÅÁöÑ...
10   ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∫îÁî®ÊòØ‰ªÄ‰πà                          HIT    1.831s    Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
11   What is machine learning?           MISS   1.965s     Machine learning is a type of artificial intelligence (AI) that enables computers to learn from dat...
12   Explain machine learning.           MISS   1.965s     What are the types of machine learning? Machine learning is a subset of artificial intelligence (AI...
13   Define machine learning.            HIT    1.829s     Machine in the States States States States States States States States States States States States ...
14   Can you describe machine learning   HIT    1.828s     Machine in the States States States States States States States States States States States States ...
15   What does machine learning mean?    HIT    1.829s     Machine in the States States States States States States States States States States States States ...
16   How would you explain machine lea   HIT    1.830s     Machine in the States States States States States States States States States States States States ...
17   What are the applications of mach   HIT    1.827s     Machine in the States States States States States States States States States States States States ...
18   Describe the concept of machine l   MISS   1.957s     How does it differ from traditional programming? Machine learning is a subfield of artificial intel...
19   What is the definition of machine   HIT    1.825s     Machine in the States States States States States States States States States States States States ...
20   Please explain the meaning of mac   HIT    1.825s     How more:// Angeles States States States States States States States States States States States St...

----------------------------------------------------------------------------------------------------

====================================================================================================
DETAILED GENERATED RESPONSES
====================================================================================================

[1] Prompt: ‰ªÄ‰πàÊòØ‰∫∫Â∑•Êô∫ËÉΩ
    Status:  Cache MISS (Full Compute)
    Latency: 1.966s
    Response: ÔºàAIÔºâÔºü
‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÊòØÊåáËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏≠ÁöÑ‰∏ÄÁßçÁ†îÁ©∂È¢ÜÂüüÔºåÊó®Âú®ÂºÄÂèëËÉΩÂ§üÊ®°Êãü„ÄÅÊâ©Â±ïÂíåË∂ÖË∂ä‰∫∫Á±ªÊô∫ËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇAIÁ≥ªÁªüÂèØ‰ª•Â≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥
--------------------------------------------------------------------------------

[2] Prompt: ‰∫∫Â∑•Êô∫ËÉΩÊòØ‰ªÄ‰πà
    Status:  Cache HIT (KV Reuse)
    Latency: 1.830s
    Response: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
--------------------------------------------------------------------------------

[3] Prompt: Ëß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑÂê´‰πâ
    Status:  Cache MISS (Full Compute)
    Latency: 1.967s
    Response: 
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàê„ÄÅËßÜËßâËØÜÂà´Á≠â„ÄÇ‰∫∫Â∑•Êô∫ËÉΩ
--------------------------------------------------------------------------------

[4] Prompt: ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆö‰πâÊòØ‰ªÄ‰πà
    Status:  Cache HIT (KV Reuse)
    Latency: 1.832s
    Response: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
--------------------------------------------------------------------------------

[5] Prompt: ËØ∑ËØ¥Êòé‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ¶ÇÂøµ
    Status:  Cache MISS (Full Compute)
    Latency: 1.959s
    Response: „ÄÅÂèëÂ±ïÂéÜÂè≤ÂíåÂ∫îÁî®Âú∫ÊôØ
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØÊåá‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂíåÊï∞Â≠¶Êù•Ê®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÂ≠¶‰π†„ÄÅÊé®ÁêÜ„ÄÅËß£ÂÜ≥ÈóÆÈ¢òÂíåÊÑüÁü•ÁéØÂ¢ÉÁ≠â„ÄÇ‰∫∫Â∑•
--------------------------------------------------------------------------------

[6] Prompt: ËÉΩÂê¶Ëß£Èáä‰∏Ä‰∏ã‰∫∫Â∑•Êô∫ËÉΩÔºü
    Status:  Cache HIT (KV Reuse)
    Latency: 1.829s
    Response: 
        in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[7] Prompt: ‰∫∫Â∑•Êô∫ËÉΩÊåáÁöÑÊòØ‰ªÄ‰πàÔºü
    Status:  Cache HIT (KV Reuse)
    Latency: 1.827s
    Response: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
--------------------------------------------------------------------------------

[8] Prompt: ËØ∑ÊèèËø∞‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰ΩúÁî®
    Status:  Cache HIT (KV Reuse)
    Latency: 1.831s
    Response: „ÄÅ BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
--------------------------------------------------------------------------------

[9] Prompt: ‰∫∫Â∑•Êô∫ËÉΩÊúâÂì™‰∫õÂ∫îÁî®Ôºü
    Status:  Cache MISS (Full Compute)
    Latency: 1.967s
    Response:  - AI Applications
‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial IntelligenceÔºåAIÔºâÊòØ‰∏ÄÁßçÊ®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑÊäÄÊúØÔºåÂÆÉÂèØ‰ª•Ëá™Âä®Â≠¶‰π†„ÄÅÊé®ÁêÜÂíåËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇAIÊäÄÊúØÊúâÂæàÂ§öÂ∫îÁî®È¢ÜÂüüÔºå‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂ∏∏ËßÅÁöÑÂ∫îÁî®Ôºö

1.
--------------------------------------------------------------------------------

[10] Prompt: ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∫îÁî®ÊòØ‰ªÄ‰πà
    Status:  Cache HIT (KV Reuse)
    Latency: 1.831s
    Response: Ôºà BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
--------------------------------------------------------------------------------

[11] Prompt: What is machine learning?
    Status:  Cache MISS (Full Compute)
    Latency: 1.965s
    Response:  Machine learning is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It involves training algorithms on large datasets to recognize patterns, make predictions, and improve their performance over time.
Machine learning is a subset of
--------------------------------------------------------------------------------

[12] Prompt: Explain machine learning.
    Status:  Cache MISS (Full Compute)
    Latency: 1.965s
    Response:  What are the types of machine learning?
Machine learning is a subset of artificial intelligence (AI) that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed. The algorithms learn by identifying patterns and relationships in the data, and
--------------------------------------------------------------------------------

[13] Prompt: Define machine learning.
    Status:  Cache HIT (KV Reuse)
    Latency: 1.829s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[14] Prompt: Can you describe machine learning?
    Status:  Cache HIT (KV Reuse)
    Latency: 1.828s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[15] Prompt: What does machine learning mean?
    Status:  Cache HIT (KV Reuse)
    Latency: 1.829s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[16] Prompt: How would you explain machine learning?
    Status:  Cache HIT (KV Reuse)
    Latency: 1.830s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[17] Prompt: What are the applications of machine learning?
    Status:  Cache HIT (KV Reuse)
    Latency: 1.827s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[18] Prompt: Describe the concept of machine learning.
    Status:  Cache MISS (Full Compute)
    Latency: 1.957s
    Response:  How does it differ from traditional programming?
Machine learning is a subfield of artificial intelligence that involves training algorithms to learn from data, without being explicitly programmed. In traditional programming, a programmer writes code that specifies exactly what the program should do in every situation
--------------------------------------------------------------------------------

[19] Prompt: What is the definition of machine learning?
    Status:  Cache HIT (KV Reuse)
    Latency: 1.825s
    Response:  Machine in the States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

[20] Prompt: Please explain the meaning of machine learning.
    Status:  Cache HIT (KV Reuse)
    Latency: 1.825s
    Response:  How more:// Angeles States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States States
--------------------------------------------------------------------------------

====================================================================================================
STATISTICS
====================================================================================================

Total Queries: 20
Cache Hits: 13
Cache Misses: 7
Hit Rate: 65.00%

Average Cache HIT Latency: 1.829s
Average Cache MISS Latency: 1.964s
Speedup (MISS/HIT): 1.07x

KV Manager Statistics:
  Cache Size: 7
  Num Buckets: 7
  Total Queries: 27
  Cache Hits: 20
  Cache Misses: 7
  Hit Rate: 74.07%

================================================================================
TEST COMPLETE
================================================================================
Ê®°ÂûãÂ∑≤ÈáäÊîæÔºåÊòæÂ≠òÂ∑≤Ê∏ÖÁêÜ
/mnt/sda1/homie_cache does not exist.
